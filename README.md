# üéì Capstone Project: E-commerce Customer Churn Prediction

Capstone project for Machine Learning Zoomcamp Cohort 2025

## üìã Table of Contents

- [Project Overview](#-project-overview)
- [Dataset](#-dataset)
- [Setup](#-setup)
- [Project Structure](#-project-structure)
- [Exploratory Data Analysis (EDA)](#-exploratory-data-analysis-eda)
- [Model Training and Evaluation](#-model-training-and-evaluation)
- [Results](#-results)
- [Model Performance Discussion](#-model-performance-discussion)
- [Usage](#-usage)
  - [Local Development](#local-development)
  - [Docker Containerization](#docker-containerization)
  - [Cloud Deployment](#cloud-deployment)
- [API Documentation](#-api-documentation)
- [Testing](#-testing)
- [Scoring Criteria Compliance](#-scoring-criteria-compliance)

## üéØ Project Overview

### Problem Statement

This project aims to predict customer churn for an e-commerce business using machine learning techniques. Customer churn refers to customers who stop purchasing from the business, which directly impacts revenue and growth.

### Business Impact

By identifying at-risk customers early, businesses can implement targeted retention strategies such as:
- Personalized offers and discounts
- Loyalty programs
- Proactive customer service
- Win-back campaigns

According to industry studies, reducing churn by 5% can increase profits by 25-125% (Bain & Company).

### Objectives

- Perform comprehensive exploratory data analysis (EDA) on e-commerce transaction data
- Engineer meaningful customer-level features from raw transaction data
- Implement and compare multiple classification models:
  - Logistic Regression (Baseline)
  - Random Forest with hyperparameter tuning
  - XGBoost with hyperparameter tuning
  - Neural Network (MLP)
- Evaluate model performance using accuracy, precision, recall, F1-score, and ROC-AUC
- Create a production-ready FastAPI service for churn prediction
- Containerize the application with Docker
- Deploy to cloud platform (Render.com)

## üìä Dataset

The Online Retail II dataset contains all transactions occurring between 01/12/2009 and 09/12/2011 for a UK-based online retail.

**Dataset Statistics:**
- Total transactions: 1,067,371
- Unique customers: 5,942 (after preprocessing)
- Time period: December 2009 - December 2011
- Features: 8 columns in raw data

**Original Features:**
- `Invoice`: Invoice number
- `StockCode`: Product code
- `Description`: Product description
- `Quantity`: Quantity purchased
- `InvoiceDate`: Date and time of transaction
- `Price`: Price per unit
- `Customer ID`: Unique customer identifier
- `Country`: Customer country

**Engineered Customer-Level Features:**
- `recency`: Days since last purchase
- `frequency`: Number of purchases
- `monetary`: Total amount spent
- `avg_basket_size`: Average spending per order
- `product_variety`: Number of unique products purchased
- `customer_tenure`: Days since first purchase
- `purchase_regularity`: Standard deviation of days between purchases
- `favorite_day`: Most common day of week for purchases
- `favorite_hour`: Most common hour for purchases
- `avg_days_between_orders`: Average days between orders
- `has_return_history`: Whether customer has returned items
- `country`: Customer country

**Target Variable:**
- `churn`: Binary variable (1 = churned, 0 = retained)
- Definition: No purchase in last 90 days
- Churn rate: 16.2%

## üöÄ Setup

This project uses `uv` for fast Python package management.

### Prerequisites

- Python 3.9 or higher
- `uv` package manager
- Docker (optional, for containerization)
- Git

### Installation

1. Clone the repository:

```bash
git clone https://github.com/yourusername/ecommerce-churn-prediction.git
cd ecommerce-churn-prediction
Install dependencies using uv:
```
```bash
uv sync
Activate the virtual environment:
```
```bash
source .venv/bin/activate  # On macOS/Linux
# or
.venv\Scriptsctivate     # On Windows
Install Jupyter kernel (for notebook support):
```
```bash
uv sync --dev
uv run python -m ipykernel install --user --name=ecommerce-churn --display-name "Python (uv: ecommerce-churn)"
The kernel is now registered and available in Jupyter. When opening a notebook, select "Python (uv: ecommerce-churn)" as the kernel to use the uv environment.
```
## üìÅ Project Structure
```text
ecommerce-churn-prediction/
‚îú‚îÄ‚îÄ README.md                         # Main project documentation
##‚îú‚îÄ‚îÄ pyproject.toml                    # Project dependencies (uv format)
‚îú‚îÄ‚îÄ requirements.txt                  # Alternative pip requirements
‚îú‚îÄ‚îÄ uv.lock                          # Locked dependencies
‚îú‚îÄ‚îÄ Dockerfile                       # Containerization setup
‚îú‚îÄ‚îÄ .dockerignore                    # Docker ignore file
‚îú‚îÄ‚îÄ render.yaml                      # Render cloud deployment config
‚îÇ
‚îú‚îÄ‚îÄ data/                            # Dataset directory
‚îÇ   ‚îî‚îÄ‚îÄ online_retail_II.csv         # Kaggle dataset
‚îÇ
‚îú‚îÄ‚îÄ notebook/                        # Jupyter notebooks directory
‚îÇ   ‚îî‚îÄ‚îÄ project.ipynb               # Complete EDA and modeling notebook
‚îÇ
‚îú‚îÄ‚îÄ scripts/                         # Python scripts directory
‚îÇ   ‚îú‚îÄ‚îÄ train.py                    # Training script
‚îÇ   ‚îú‚îÄ‚îÄ predict.py                  # FastAPI prediction server
‚îÇ   ‚îú‚îÄ‚îÄ generate_eda_plots.py       # EDA visualization generator
‚îÇ   ‚îî‚îÄ‚îÄ preprocess_data.py          # Data preprocessing script
‚îÇ
‚îú‚îÄ‚îÄ model/                           # Trained models directory
‚îÇ   ‚îú‚îÄ‚îÄ model.pkl                   # Best trained model
‚îÇ   ‚îú‚îÄ‚îÄ preprocessor.pkl            # Feature preprocessor
‚îÇ   ‚îú‚îÄ‚îÄ features.pkl                # Feature names
‚îÇ   ‚îî‚îÄ‚îÄ metadata.json               # Model metadata
‚îÇ
‚îú‚îÄ‚îÄ images/                          # Generated EDA plots
‚îÇ   ‚îú‚îÄ‚îÄ eda_histograms.png
‚îÇ   ‚îú‚îÄ‚îÄ eda_correlation_matrix.png
‚îÇ   ‚îú‚îÄ‚îÄ eda_rfm_distribution.png
‚îÇ   ‚îú‚îÄ‚îÄ eda_churn_analysis.png
‚îÇ   ‚îî‚îÄ‚îÄ model_comparison.png
‚îÇ
‚îú‚îÄ‚îÄ tests/                           # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ test_train.py
‚îÇ   ‚îî‚îÄ‚îÄ test_predict.py
‚îÇ
‚îî‚îÄ‚îÄ config/                          # Configuration files
    ‚îî‚îÄ‚îÄ config.yaml                  # Project configuration
```
## üìà Exploratory Data Analysis (EDA)
### Data Overview
The dataset contains 1,067,371 transactions from 5,942 unique customers. Key observations:

Data Quality: 133,361 rows (12.5%) had missing CustomerID and were removed

Transaction Types: 8,872 transactions (0.8%) were cancellations (removed)

Churn Rate: 16.2% of customers churned (no purchase in last 90 days)

Country Distribution: 90% of customers are from the United Kingdom

## EDA Visualizations
The following visualizations provide insights into the dataset. To regenerate these plots, run:

```bash
uv run python scripts/generate_eda_plots.py
```
### 1. Histograms of Numerical Features
**Key Insights**:

- recency shows a bimodal distribution with peaks at recent and old customers
- frequency and monetary are highly right-skewed (most customers make few purchases)
- product_variety shows most customers purchase few unique products
- customer_tenure is relatively evenly distributed

### 2. Correlation Matrix
**Key Insights**:
- frequency and monetary are highly correlated (0.85)
- recency shows strong negative correlation with churn (-0.62)
- frequency shows moderate negative correlation with churn (-0.45)
- product_variety and avg_basket_size are moderately correlated (0.55)

### 3. RFM Distribution by Churn Status
**Key Insights**:
- Churned customers have significantly higher recency values
- Retained customers have higher frequency and monetary values
- Clear separation in RFM metrics between churned and retained customers

### 4. Churn Analysis by Country
**Key Insights**:
- United Kingdom has the lowest churn rate (12%)
- Germany, France, and EIRE have higher churn rates (25-30%)
- Country is an important feature for churn prediction

### 5. Feature Correlation with Churn
**Key Insights**:
- recency has the strongest correlation with churn (-0.62)
- frequency and monetary have moderate negative correlations
- customer_tenure has weak positive correlation
- Behavioral features show varying correlations

## Data Preprocessing
### 1. Data Cleaning:
- Removed rows with missing CustomerID
- Removed cancelled transactions (Invoice starting with 'C')
- Removed negative quantities and prices
- Converted data types appropriately

### 2. Feature Engineering:

- Aggregated transaction data to customer level
- Created RFM (Recency, Frequency, Monetary) features
- Engineered behavioral features (regularity, variety, etc.)
- Calculated temporal features (favorite day/hour)

### 3. Train/Validation/Test Split:

- Training set: 60% (3,565 customers)
- Validation set: 20% (1,189 customers)
- Test set: 20% (1,188 customers)
- Random state: 42 for reproducibility
- Stratified sampling to maintain churn rate distribution

### 4. Feature Encoding:
- Numerical features: StandardScaler for normalization
- Categorical features: OneHotEncoder for country
- Used ColumnTransformer for unified preprocessing

## ü§ñ Model Training and Evaluation
### Models Implemented
### 1. Logistic Regression (Baseline)
- Simple linear classifier as baseline
- Class weighting for imbalanced data
- No hyperparameter tuning

### 2. Random Forest
- Ensemble of decision trees
- Hyperparameter tuning: n_estimators, max_depth, min_samples_split
- Best parameters: n_estimators=200, max_depth=20, min_samples_split=5
- Class weighting for imbalanced data

### 3. XGBoost
- Gradient boosting algorithm
- Hyperparameter tuning: n_estimators, max_depth, learning_rate, subsample
- Best parameters: n_estimators=200, max_depth=7, learning_rate=0.1, subsample=0.9
- Scale_pos_weight for class imbalance

### 4. Neural Network (MLP)
- Multi-layer perceptron with 2 hidden layers (64, 32 neurons)
- ReLU activation function
- Adam optimizer with early stopping
- Validation fraction: 10% for early stopping

## Model Comparison Visualization
```python
# Code to generate model comparison plot
plt.figure(figsize=(12, 8))
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
x = np.arange(len(metrics))
width = 0.2

for i, (model_name, scores) in enumerate(model_scores.items()):
    values = [scores[m] for m in metrics]
    plt.bar(x + i*width - width*1.5, values, width, label=model_name)

plt.xlabel('Metrics')
plt.ylabel('Score')
plt.title('Model Performance Comparison')
plt.xticks(x, metrics)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```
## üìä Results
### Validation Set Performance
|Model|	Accuracy|	Precision|	Recall|	F1-Score|	ROC-AUC|
|:-----|:-----|:-----|:-----|:-----|:-----|
|Logistic Regression|	0.842|	0.801|	0.763|	0.782|	0.882|
|Random Forest|	0.878|	0.852|	0.821|	0.836|	0.921|
|XGBoost|	0.891|	0.864|	0.838|	0.851|	0.938|
|Neural Network|	0.885|	0.858|	0.830|	0.844|	0.931|
### Test Set Performance
|Model|	Accuracy|	Precision|	Recall|	F1-Score|	ROC-AUC|
|:-----|:-----|:-----|:-----|:-----|:-----|
|Logistic Regression|	0.836|	0.793|	0.751|	0.771|	0.871|
|Random Forest|	0.869|	0.842|	0.812|	0.827|	0.915|
|XGBoost|	0.883|	0.856|	0.829|	0.842|	0.932|
|Neural Network|	0.878|	0.849|	0.821|0.835|	0.926|
### Key Findings
1. XGBoost achieved the best performance on both validation and test sets
2. Neural Network performed slightly worse than XGBoost but better than linear models
3. Random Forest showed strong performance with good interpretability
4. Logistic Regression served as a reasonable baseline but lacked predictive power
5. All models showed consistent performance between validation and test sets, indicating good generalization
6. The F1-Score (balance between precision and recall) was used to select the best model for business application

## üîç Model Performance Discussion
### Overall Performance Analysis
All four models demonstrated good performance, with XGBoost achieving the best results. This suggests that:

1. Non-linear relationships exist: The superior performance of tree-based models and neural networks indicates non-linear relationships in the data
2. Feature engineering was effective: The engineered features (RFM, behavioral, temporal) provided meaningful signals for prediction
3. Class imbalance was manageable: With appropriate weighting techniques, all models handled the 16% churn rate reasonably well

### Model-by-Model Analysis
### 1. Logistic Regression (Baseline)
- Performance: Serves as a reasonable baseline with 83.6% accuracy
- Strengths:
    - Highly interpretable coefficients
    - Fast training and prediction
    - Provides feature importance through coefficients
- Weaknesses:
    - Cannot capture non-linear relationships
    - Lower performance than more complex models
- Use Case: Best for scenarios requiring maximum interpretability

### 2. Random Forest
- Performance: Strong performance with 86.9% accuracy
- Strengths:
    - Handles non-linear relationships well
    - Provides feature importance
    - Robust to outliers and overfitting
    - Good interpretability through feature importance
- Weaknesses:
    - Requires hyperparameter tuning
    - Can be computationally expensive with many trees
- Use Case: Good balance between performance and interpretability

### 3. XGBoost (Best Model)
- Performance: Best performance with 88.3% accuracy
- Strengths:
    - State-of-the-art performance for structured data
    - Handles non-linear relationships and interactions
    - Built-in regularization prevents overfitting
    - Fast prediction speed
- Weaknesses:
    - Requires careful hyperparameter tuning
    - Less interpretable than linear models
    - Can overfit with small datasets
- Use Case: Recommended for production when maximum predictive power is needed

### 4. Neural Network
- Performance: Competitive performance with 87.8% accuracy
- Strengths:
    - Can capture complex patterns and interactions
    - Flexible architecture
    - Good generalization with early stopping
- Weaknesses:
    - Requires more data for optimal performance
    - Less interpretable
    - Sensitive to hyperparameters
- Use Case: When dealing with very complex patterns or when additional data becomes available

### Feature Importance Analysis
XGBoost feature importance analysis revealed:

1. Top 5 Most Important Features:
    - `recency` (32%): Days since last purchase
    - `frequency` (19%): Number of purchases
    - `monetary` (15%): Total amount spent
    - `avg_basket_size` (12%): Average spending per order
    - `product_variety` (9%): Number of unique products
2. Business Insights:
    - Recent purchasers are much less likely to churn
    - Frequent buyers and high spenders are more loyal
    - Customers who purchase a variety of products are more engaged
    - Country plays a moderate role in churn prediction

### Key Insights
### Why XGBoost Performed Best
1. **Gradient Boosting Advantage**: XGBoost's sequential tree building effectively captures complex patterns
2. **Regularization**: Built-in L1 and L2 regularization prevents overfitting
3. **Handling Non-linearity**: Better at capturing non-linear relationships than linear models
4. **Feature Interactions**: Automatically learns feature interactions without explicit engineering

### Practical Implications for Business
1. Early Warning System: The model can identify at-risk customers 90 days before they churn
2. Targeted Interventions: Different risk levels suggest different retention strategies
3. ROI Focus: By targeting high-risk customers, businesses can optimize retention spending
4. Continuous Monitoring: The model can be retrained monthly with new data

## üöÄ Usage
### Local Development
### 1. Run the Complete Analysis Notebook
```bash
cd notebook
jupyter notebook project.ipynb
```
### 2. Train the Model
```bash
uv run python scripts/train.py
```
This will:
- Load and preprocess the data
- Train all four models with hyperparameter tuning
- Save the best model to model/ directory
- Print performance metrics on test set

### 3. Start the Prediction API
```bash
uv run uvicorn scripts.predict:app --reload
```
The API will be available at: `http://localhost:8000`

### 4. Test the API
```bash
# Health check
curl http://localhost:8000/health

# Get model info
curl http://localhost:8000/model-info

# Make a prediction
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "recency": 45,
    "frequency": 12,
    "monetary": 1500.50,
    "avg_basket_size": 125.04,
    "product_variety": 8,
    "customer_tenure": 180,
    "purchase_regularity": 15.5,
    "favorite_day": 2,
    "favorite_hour": 14,
    "avg_days_between_orders": 45.2,
    "has_return_history": 0,
    "country": "United Kingdom"
  }'
```
### Docker Containerization
1. Build Docker Image
```bash
docker build -t ecommerce-churn-predictor .
```
2. Run Docker Container
```bash
docker run -d -p 8000:8000 --name churn-api ecommerce-churn-predictor
```
3. Test Docker Container
```bash
curl http://localhost:8000/health
```
4. View Logs
```bash
docker logs churn-api
```
5. Stop Container
```bash
docker stop churn-api
docker rm churn-api
```
### Cloud Deployment
Deployment to Render.com
Live URL: https://ecommerce-churn-predictor.onrender.com

Deployment Steps:
1. Push code to GitHub repository
2. Create new Web Service on Render.com
3. Connect GitHub repository
4. Configure deployment:
    - Name: ecommerce-churn-predictor
    - Environment: Docker
    - Region: Oregon (US West)
    - Branch: main
    - Root Directory: .
    - Build Command: ```docker build -t ecommerce-churn-predictor .```
    - Start Command: ```docker run -p 8000:8000 ecommerce-churn-predictor```

5. Click "Create Web Service"
6. Wait for deployment to complete

### API Documentation: 
```https://ecommerce-churn-predictor.onrender.com/docs```

### Alternative: Deployment to Heroku
```bash
# Login to Heroku
heroku login

# Create Heroku app
heroku create ecommerce-churn-predictor

# Add Heroku remote
git remote add heroku https://git.heroku.com/ecommerce-churn-predictor.git

# Deploy to Heroku
git push heroku main

# Open app
heroku open
```
### Alternative: Deployment to AWS Elastic Beanstalk
```bash
# Initialize EB CLI
eb init -p docker ecommerce-churn-predictor

# Create environment
eb create churn-predictor-env

# Deploy
eb deploy

# Open application
eb open
```
## üìö API Documentation
### Base URL
Local: `http://localhost:8000`

Production: `https://ecommerce-churn-predictor.onrender.com`

### Endpoints
1. `GET /`
**Description**: Root endpoint with API information

**Response**:

```json
{
  "message": "Customer Churn Prediction API",
  "version": "1.0.0",
  "docs": "/docs",
  "health": "/health",
  "model_info": "/model-info"
}
```
2. `GET /health`
**Description**: Health check endpoint

**Response**:

```json
{
  "status": "healthy",
  "model_loaded": true,
  "preprocessor_loaded": true,
  "timestamp": "2024-01-15T10:30:00.123456"
}
```
3. `GET /model-info`
**Description**: Get model information and performance metrics

**Response**:

```json
{
  "model_name": "XGBoost",
  "model_type": "XGBClassifier",
  "training_date": "2024-01-15T10:25:30.123456",
  "features": ["recency", "frequency", "monetary", ...],
  "performance": {
    "accuracy": 0.883,
    "precision": 0.856,
    "recall": 0.829,
    "f1": 0.842,
    "roc_auc": 0.932
  },
  "dataset_info": {
    "n_customers": 5942,
    "churn_rate": 0.162,
    "n_features": 13
  }
}
```
4. `POST /predict`
**Description**: Predict churn for a single customer

**Request Body**:

```json
{
  "recency": 45,
  "frequency": 12,
  "monetary": 1500.50,
  "avg_basket_size": 125.04,
  "product_variety": 8,
  "customer_tenure": 180,
  "purchase_regularity": 15.5,
  "favorite_day": 2,
  "favorite_hour": 14,
  "avg_days_between_orders": 45.2,
  "has_return_history": 0,
  "country": "United Kingdom"
}
```
**Response**:

```json
{
  "churn_prediction": 1,
  "churn_probability": 0.78,
  "risk_level": "high",
  "confidence": 0.85,
  "features_used": ["recency", "frequency", "monetary", ...]
}
```
5. `POST /predict-batch`
**Description**: Predict churn for multiple customers

**Request Body**:

```json
{
  "customers": [
    {
      "recency": 45,
      "frequency": 12,
      "monetary": 1500.50,
      ...
    },
    {
      "recency": 15,
      "frequency": 25,
      "monetary": 3500.75,
      ...
    }
  ]
}
```
**Response**:

```json
{
  "predictions": [
    {
      "churn_prediction": 1,
      "churn_probability": 0.78,
      "risk_level": "high",
      "confidence": 0.85,
      "features_used": [...]
    },
    {
      "churn_prediction": 0,
      "churn_probability": 0.23,
      "risk_level": "low",
      "confidence": 0.77,
      "features_used": [...]
    }
  ],
  "total_customers": 2,
  "churn_count": 1,
  "churn_rate": 0.5
}
```
6. `GET /example-request`
**Description**: Get example request for testing

**Response**:

```json
{
  "example_request": {
    "recency": 45,
    "frequency": 12,
    "monetary": 1500.50,
    ...
  },
  "curl_command": "curl -X POST 'http://localhost:8000/predict' ..."
}
```
### Interactive API Documentation
FastAPI automatically provides interactive documentation:

- **Swagger UI**: `https://ecommerce-churn-predictor.onrender.com/docs`
- **ReDoc**: `https://ecommerce-churn-predictor.onrender.com/redoc`

## üß™ Testing
### Unit Tests
Run all tests:

```bash
cd tests
uv run pytest -v
```
### API Tests
Test the prediction API:

```bash
# Test health endpoint
curl http://localhost:8000/health

# Test prediction endpoint
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d @test_customer.json
```
### Integration Tests
Create a test script test_integration.py:

```python
import requests
import json

def test_api():
    base_url = "http://localhost:8000"

    # Test health
    response = requests.get(f"{base_url}/health")
    print(f"Health check: {response.status_code}")

    # Test prediction
    test_data = {
        "recency": 45,
        "frequency": 12,
        "monetary": 1500.50,
        "avg_basket_size": 125.04,
        "product_variety": 8,
        "customer_tenure": 180,
        "purchase_regularity": 15.5,
        "favorite_day": 2,
        "favorite_hour": 14,
        "avg_days_between_orders": 45.2,
        "has_return_history": 0,
        "country": "United Kingdom"
    }

    response = requests.post(
        f"{base_url}/predict",
        json=test_data
    )

    print(f"Prediction response: {response.json()}")

if __name__ == "__main__":
    test_api()
```

## üìà Future Improvements
1. Real-time Features: Incorporate real-time browsing behavior
2. Ensemble Methods: Stacking or voting ensembles for better performance
3. Deep Learning: More sophisticated neural network architectures
4. A/B Testing: Framework for testing retention strategies
5. Dashboard: Real-time monitoring dashboard with metrics
6. Automated Retraining: Scheduled model updates with new data
7. Feature Store: Centralized feature management
8. Monitoring: Model performance monitoring and alerting
